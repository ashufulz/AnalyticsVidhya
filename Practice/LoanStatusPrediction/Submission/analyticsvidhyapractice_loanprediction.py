# -*- coding: utf-8 -*-
"""AnalyticsVidhyaPractice_LoanPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DPiTzjaGom77GJj7ihfo7aTPCFDaud7s

#Predict Loan Eligibility for Dream Housing Finance company

Dream Housing Finance company deals in all kinds of home loans. They have presence across all urban, semi urban and rural areas. Customer first applies for home loan and after that company validates the customer eligibility for loan.

Company wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have provided a dataset to identify the customers segments that are eligible for loan amount so that they can specifically target these customers.
"""

# Importing required basic libraries

import numpy as np
import pandas as pd

# Loading Train dataset
df_train = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/_CHALLENGES_/AnalyticsVidhya/PRACTICE/LoanPrediction/AV_train_ctrUa4K.csv')

"""# EDA"""

df_train.shape

df_train.head()

# Dropping the irrelevant ID column
df_train.drop(['Loan_ID'], axis=1, inplace=True)

df_train.info()

df_train.describe()

"""# DATA PREPROCESSING

## These are the categorical columns which we will need to convert into numerical forms through any of the encoding techniques.
"""

df_train.select_dtypes(object).columns

"""### Let's see the values of each colummn one by one to understand the data better"""

print('Gender:', df_train.Gender.unique(), df_train.Gender.nunique())
print('Married:', df_train.Married.unique(), df_train.Married.nunique())
print('Dependents:', df_train.Dependents.unique(), df_train.Dependents.nunique())
print('Education:', df_train.Education.unique(), df_train.Education.nunique())
print('Self_Employed:', df_train.Self_Employed.unique(), df_train.Self_Employed.nunique())
print('Property_Area:', df_train.Property_Area.unique(), df_train.Property_Area.nunique())
print('Loan_Status:', df_train.Loan_Status.unique(), df_train.Loan_Status.nunique())

"""### As seen we can see that there are 'nan' values also, which can be missing values."""

df_train.isna().sum()

"""## Now we will fill the nan values according to the columns

### *Below are the categorical columns, hence we will be replacing the missing values with the mode of the respective columns*
"""

df_train.Gender = df_train.Gender.fillna(df_train.Gender.mode()[0])
df_train.Married = df_train.Married.fillna(df_train.Married.mode()[0])
df_train.Dependents = df_train.Dependents.fillna(df_train.Dependents.mode()[0])
df_train.Self_Employed = df_train.Self_Employed.fillna(df_train.Self_Employed.mode()[0])
df_train.Loan_Amount_Term = df_train.Loan_Amount_Term.fillna(df_train.Loan_Amount_Term.mode()[0])
df_train.Credit_History = df_train.Credit_History.fillna(df_train.Credit_History.mode()[0])

"""### *Below is the numerical column where we will be replacing the missing values with the mean of that column*"""

df_train.LoanAmount = df_train.LoanAmount.fillna(df_train.LoanAmount.mean())

"""### As the categorical columns are random, we can't go with LabelEncoding on it.
###We'll use OneHotEncoding
Nominal data -> data is random -> OneHotEncoding
"""

df_train.head()

# One hot encoding on categorical columns.
df_train = pd.get_dummies(df_train, columns=['Gender','Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents', 'Loan_Amount_Term'], drop_first=True)

"""### *Replacing Y/N with 1/0 respectively*"""

df_train.Loan_Status = df_train.Loan_Status.replace('Y', '1')
df_train.Loan_Status = df_train.Loan_Status.replace('N', '0')

"""### *Here datatype of Loan_Status would still be object, so we need to convert it into numerical form*"""

df_train.Loan_Status = df_train.Loan_Status.astype(int)

"""### Scaling the data"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

cols_to_scale = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']
df_train[cols_to_scale] = sc.fit_transform(df_train[cols_to_scale])

# Importing Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing the correlation between the features

plt.figure(figsize=(25, 25))
sns.heatmap(df_train.corr(), annot=True, cmap='Blues')

"""# DEFINING THE VARIABLES"""

df_train.columns

X = df_train.drop(['Loan_Status'], axis=1)        # Independant Variable / Features
y = df_train['Loan_Status']                       # Dependant Variable / Target

"""## Getting important features"""

from sklearn.ensemble import ExtraTreesRegressor

model = ExtraTreesRegressor()
model.fit(X, y)

# Getting the important features
model.feature_importances_

# Visualizing Important Features
plt.figure(figsize=(12,8))
impft = pd.Series(model.feature_importances_, index=X.columns)
impft.nlargest(20).plot(kind='barh')
plt.show()

"""### So we can finally say that Credit History of the person plays an important role whether he/she is eligible for loan or not. Along with that the person's income, the co-partner's income(incase of joint account) and lastly the Loan Amount is also very much important.

# **BUILDING MODEL**
---
"""

# Dividing the data into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)      # 80% train data, 20% test data

# As only 'Accuracy' value will be used to judge the results, we'll import that
# from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

# To find best parameters
from sklearn.model_selection import GridSearchCV

"""# LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression

log = LogisticRegression(random_state=42)

penalty = ['l1', 'l2', 'elasticnet', 'none']
class_weight = [None, 'balanced']
solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
max_iter = np.arange(100, 500, 100)
multi_class = ['auto', 'ovr', 'multinomial']

log_params = dict(penalty = penalty,
                  class_weight = class_weight,
                  solver = solver,
                  max_iter = max_iter,
                  multi_class = multi_class)

"""### *Finding the best params*"""

mod_gs_log = GridSearchCV(estimator=log, param_grid=log_params, verbose=1, n_jobs=-1, cv=10)
mod_gs_log.fit(X_train, y_train)
bp_log = mod_gs_log.best_params_
bp_log

"""### *Training with best parameters*"""

mod_log = LogisticRegression(class_weight=bp_log['class_weight'], max_iter=bp_log['max_iter'], multi_class=bp_log['multi_class'], 
                             penalty=bp_log['penalty'], solver=bp_log['solver'], random_state=42)

# Fitting the model
mod_log.fit(X_train, y_train)

# Storing the prediction
ypred_log = mod_log.predict(X_test)

# Accuracy for this model
acc_log = accuracy_score(y_test, ypred_log)
print('Acc Log:', acc_log)

"""# DECISION TREE"""

from sklearn.tree import DecisionTreeClassifier

dtc = DecisionTreeClassifier(random_state=42)

criterion = ['gini', 'entropy']
splitter = ['best', 'random']
max_depth = np.arange(1, 11, 2)
min_samples_split = np.arange(2, 10, 2)
min_samples_leaf = np.arange(2, 10, 2)
max_features = [None, 'auto', 'sqrt', 'log2']

dtc_params = dict(criterion = criterion,
                  splitter = splitter,
                  max_depth = max_depth,
                  min_samples_split = min_samples_split,
                  min_samples_leaf = min_samples_leaf,
                  max_features = max_features)

"""### *Finding the best params*"""

mod_gs_dtc = GridSearchCV(estimator = dtc, param_grid = dtc_params, cv=10, verbose = 1, n_jobs = -1)
mod_gs_dtc.fit(X_train, y_train)
bp_dtc = mod_gs_dtc.best_params_
bp_dtc

"""### *Training with best parameters*"""

mod_dtc = DecisionTreeClassifier(max_depth=bp_dtc['max_depth'], min_samples_leaf=bp_dtc['min_samples_leaf'], criterion=bp_dtc['criterion'],
                                 max_features=bp_dtc['max_features'], min_samples_split=bp_dtc['min_samples_split'], splitter=bp_dtc['splitter'])

# Fitting the model
mod_dtc.fit(X_train, y_train)

# Storing the prediction
ypred_dtc = mod_dtc.predict(X_test)

# Accuracy for this model
acc_dtc = accuracy_score(y_test, ypred_dtc)
print('Acc DTC:', acc_dtc)

"""# SGD CLASSIFIER"""

from sklearn.linear_model import SGDClassifier

sgd = SGDClassifier(random_state=42)

loss = ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron']
penalty = ['l1', 'l2', 'elasticnet']
alpha = [0.001, 0.01, 0.1, 1, 10, 100]
learning_rate = ['constant', 'optimal', 'invscaling', 'adaptive']
eta0 = [1, 10, 100]
max_iter = np.arange(200, 1000, 200)

sgd_params = dict(loss = loss,
                  penalty = penalty,
                  alpha = alpha,
                  learning_rate = learning_rate, 
                  class_weight = class_weight, 
                  eta0 = eta0,
                  max_iter = max_iter)

"""### *Finding the best params*"""

mod_gs_sgd = GridSearchCV(estimator=sgd, param_grid=sgd_params, verbose=1, n_jobs=-1, cv=10)
mod_gs_sgd.fit(X_train, y_train)
bp_sgd = mod_gs_sgd.best_params_
bp_sgd

"""### *Training with best parameters*"""

mod_sgd = SGDClassifier(loss=bp_sgd['loss'], penalty=bp_sgd['penalty'], max_iter=bp_sgd['max_iter'], learning_rate=bp_sgd['learning_rate'],
                        eta0=bp_sgd['eta0'], alpha=bp_sgd['alpha'])

# Fitting the model
mod_sgd.fit(X_train, y_train)

# Storing the prediction
ypred_sgd = mod_sgd.predict(X_test)

# Accuracy for this model
acc_sgd = accuracy_score(y_test, ypred_sgd)
print('Acc SGD:', acc_sgd)

"""# RIDGE CLASSIFIER"""

from sklearn.linear_model import RidgeClassifier

rid = RidgeClassifier(random_state=42)

alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]
solver = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']
class_weight = ['balanced', None]
max_iter = np.arange(200, 2000, 200)

rid_params = dict(alpha = alpha,
                  max_iter = max_iter,
                  class_weight = class_weight,
                  solver = solver)

"""### *Finding the best params*"""

mod_gs_rid = GridSearchCV(estimator = rid, param_grid = rid_params, verbose = 1, n_jobs = -1, cv=10)
mod_gs_rid.fit(X_train, y_train)
bp_rid = mod_gs_rid.best_params_
bp_rid

"""### *Training with best parameters*"""

mod_rid = RidgeClassifier(alpha=bp_rid['alpha'], class_weight=bp_rid['class_weight'], max_iter=bp_rid['max_iter'], solver=bp_rid['solver'])

# Fitting the model
mod_rid.fit(X_train, y_train)

# Storing the prediction
ypred_rid = mod_rid.predict(X_test)

# Accuracy for this model
acc_rid = accuracy_score(y_test, ypred_rid)
print('Acc RID:', acc_rid)

"""# K-NEAREST NEIGHBOR"""

from sklearn.neighbors import KNeighborsClassifier

knc = KNeighborsClassifier()

n_neighbors = np.arange(1, 21, 2)
weights = ['uniform', 'distance']
algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']
leaf_size = np.arange(0, 50, 5)
metric = ['euclidean', 'manhattan', 'minkowski']

knc_params = dict(n_neighbors = n_neighbors,
                  weights = weights,
                  algorithm = algorithm,
                  leaf_size = leaf_size,
                  metric = metric)

"""### *Finding the best params*"""

mod_gs_knc = GridSearchCV(estimator = knc, param_grid = knc_params, verbose = 1, n_jobs = -1, cv=10)
mod_gs_knc.fit(X_train, y_train)
bp_knc = mod_gs_knc.best_params_
bp_knc

"""### *Training with best params*"""

mod_knc = KNeighborsClassifier(algorithm=bp_knc['algorithm'], leaf_size=bp_knc['leaf_size'], metric=bp_knc['metric'], n_neighbors=bp_knc['n_neighbors'],
                          weights=bp_knc['weights'])

# Fitting the model
mod_knc.fit(X_train, y_train)

# Storing the prediction
ypred_knc = mod_knc.predict(X_test)

# Accuracy for this model
acc_knc = accuracy_score(y_test, ypred_knc)
print('Acc KNC:', acc_knc)

"""# RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(random_state=42)

n_estimators = [2, 4, 8, 16, 32, 64, 100]
max_features = ['sqrt', 'log2', 'auto']
criterion = ['gini', 'entropy']
max_depth = np.arange(1, 16, 2)
min_samples_split = [0.01, 0.1, 1.0]
min_samples_leaf = [0.01, 0.1, 1.0]
class_weight = ['balanced', 'balanced_subsample']

rfc_params = dict(n_estimators = n_estimators,
                  max_features = max_features,
                  criterion = criterion,
                  max_depth = max_depth,
                  min_samples_split = min_samples_split,
                  min_samples_leaf = min_samples_leaf,
                  class_weight = class_weight)

"""### *Finding the best params*"""

mod_gs_rfc = GridSearchCV(estimator = rfc, param_grid = rfc_params, verbose = 1, n_jobs = -1, cv=5)
mod_gs_rfc.fit(X_train, y_train)
bp_rfc = mod_gs_rfc.best_params_
bp_rfc

"""### *Training with best params*"""

mod_rfc = RandomForestClassifier(n_estimators=bp_rfc['n_estimators'], max_features=bp_rfc['max_features'], criterion=bp_rfc['criterion'],
                  max_depth=bp_rfc['max_depth'], min_samples_split=bp_rfc['min_samples_split'], min_samples_leaf=bp_rfc['min_samples_leaf'],
                  class_weight=bp_rfc['class_weight'])

# Fitting the model
mod_rfc.fit(X_train, y_train)

# Storing the prediction
ypred_rfc = mod_rfc.predict(X_test)

# Accuracy for this model
acc_rfc = accuracy_score(y_test, ypred_rfc)
print('Acc RFC:', acc_rfc)

"""# ADA-BOOST CLASSIFIER"""

from sklearn.ensemble import AdaBoostClassifier

abc = AdaBoostClassifier(random_state=42)

algorithm = ['SAMME', 'SAMME.R']
n_estimators = [10, 50, 100, 200]
learning_rate = [10, 1, 0.1, 0.01, 0.001]

abc_params = dict(algorithm = algorithm,
                 n_estimators = n_estimators,
                 learning_rate = learning_rate)

"""### *Finding the best params*"""

mod_gs_abc = GridSearchCV(estimator = abc, param_grid = abc_params, verbose = 1, n_jobs = -1, cv=10)
mod_gs_abc.fit(X_train, y_train)
bp_abc = mod_gs_abc.best_params_
bp_abc

"""### *Training with best params*"""

mod_abc = AdaBoostClassifier(algorithm=bp_abc['algorithm'], n_estimators=bp_abc['n_estimators'], learning_rate=bp_abc['learning_rate'])

# Fitting the model
mod_abc.fit(X_train, y_train)

# Storing the prediction
ypred_abc = mod_abc.predict(X_test)

# Accuracy for this model
acc_abc = accuracy_score(y_test, ypred_abc)
print('Acc ABC:', acc_abc)

"""# SUPPORT VECTOR CLASSIFIER"""

from sklearn.svm import SVC

svc = SVC(random_state=42)

C = [100, 10, 1.0, 0.1, 0.01]
kernel = ['linear', 'poly', 'rbf', 'sigmoid']
gamma = ['scale', 'auto']

svc_params = dict(C = C,
                  kernel = kernel,
                  gamma = gamma)

"""### *Finding the best params*"""

mod_gs_svc = GridSearchCV(estimator = svc, param_grid = svc_params, verbose = 1, n_jobs = -1, cv=10)
mod_gs_svc.fit(X_train, y_train)
bp_svc = mod_gs_svc.best_params_
bp_svc

"""### *Training with best params*"""

mod_svc = SVC(C=bp_svc['C'], kernel=bp_svc['kernel'], gamma=bp_svc['gamma'])

# Fitting the model
mod_svc.fit(X_train, y_train)

# Storing the prediction
ypred_svc = mod_svc.predict(X_test)

# Accuracy for this model
acc_svc = accuracy_score(y_test, ypred_svc)
print('Acc SVC:', acc_svc)

"""# CATEGORICAL NAIVE BAYES"""

from sklearn.naive_bayes import GaussianNB

mod_gnb = GaussianNB()

# Fitting the model
mod_gnb.fit(X_train, y_train)

# Storing the prediction
ypred_gnb = mod_gnb.predict(X_test)

# Accuracy for this model
acc_gnb = accuracy_score(y_test, ypred_gnb)
print('Acc GNB:', acc_gnb)

"""# BAGGING CLASSIFIER"""

from sklearn.ensemble import BaggingClassifier

bc = BaggingClassifier(random_state=42)

n_estimators = [1, 10, 20, 50, 100]

bc_params = dict(n_estimators = n_estimators)

"""### *Finding the best params*"""

mod_gs_bc = GridSearchCV(estimator = bc, param_grid = bc_params, cv = 10, verbose = 1, n_jobs = -1)
mod_gs_bc.fit(X_train, y_train)
bp_bc = mod_gs_bc.best_params_
bp_bc

"""### *Training with best parameters*"""

mod_bc = BaggingClassifier(base_estimator=dtc, n_estimators=bp_bc['n_estimators'], random_state=42)

# Fitting the model
mod_bc.fit(X_train, y_train)

# Storing the prediction
ypred_bc = mod_bc.predict(X_test)

# Accuracy for this model
acc_bc = accuracy_score(y_test, ypred_bc)
print('Acc BC:', acc_bc)

"""### *Comparing all the accuracy scores:*"""

print('ABC: ', acc_abc)
print(' BC: ', acc_bc)
print('DTC: ', acc_dtc)
print('GNB: ', acc_gnb)
print('KNC: ', acc_knc)
print('LOG: ', acc_log)
print('RFC: ', acc_rfc)
print('RID: ', acc_rid)
print('SGD: ', acc_sgd)
print('SVC: ', acc_svc)

"""### *Here we have 2 algorithms gving maximum accuracies. Hence we can use anyone but we will go with Random Forest*"""

# Saving the RFC model
import pickle

rfc_model = 'rfc_model.sav'
pickle.dump(mod_rfc, open(rfc_model, 'wb'))

"""# **TEST DATA**
---
"""

df_test = pd.read_csv('/content/drive/MyDrive/DataScience/Dataset/_CHALLENGES_/AnalyticsVidhya/PRACTICE/LoanPrediction/AV_test_lAUu6dG.csv')

df_test.head()

df = pd.DataFrame(data=df_test.Loan_ID)

# Dropping the irrelevant ID column
df_test.drop(['Loan_ID'], axis=1, inplace=True)

"""### *For categorical columns, we will be replacing the missing values with the mode of the respective columns and for Numerical columns we will replace the missing values with the mean of that column*"""

df_test.Gender = df_test.Gender.fillna(df_test.Gender.mode()[0])
df_test.Married = df_test.Married.fillna(df_test.Married.mode()[0])
df_test.Dependents = df_test.Dependents.fillna(df_test.Dependents.mode()[0])
df_test.Self_Employed = df_test.Self_Employed.fillna(df_test.Self_Employed.mode()[0])
df_test.Loan_Amount_Term = df_test.Loan_Amount_Term.fillna(df_test.Loan_Amount_Term.mode()[0])
df_test.Credit_History = df_test.Credit_History.fillna(df_test.Credit_History.mode()[0])

df_test.LoanAmount = df_test.LoanAmount.fillna(df_test.LoanAmount.mean())

# One hot encoding on categorical columns.
df_test = pd.get_dummies(df_test, columns=['Gender','Married', 'Education', 'Self_Employed', 'Property_Area', 'Dependents', 'Loan_Amount_Term'], drop_first=True)

# Loading the saved RFC model
rfc_loaded = pickle.load(open(rfc_model, 'rb'))

print(df_train.columns)
print('-'*50)
print(df_test.columns)

# Test data has 2 extra columns and removing it would not make any difference to the data. Hence dropping it.
df_test.drop(['Loan_Amount_Term_12.0', 'Loan_Amount_Term_350.0'], axis=1, inplace=True)

# Testing on the Test Dataset
test_rfc_loaded = rfc_loaded.fit(X_train, y_train)
test_ypred = test_rfc_loaded.predict(df_test)

df_test['Loan_Status'] = test_ypred

df_test['Loan_Status'].value_counts()

"""### *Here the Status is stored in 1 and 0, so we will replace it to get Y / N*"""

df_test.Loan_Status = df_test.Loan_Status.replace('1', 'Y')
df_test.Loan_Status = df_test.Loan_Status.replace('0', 'N')

"""### *Adding back the dropped 'Loan_ID' to the test dataset*"""

df_test['Loan_ID'] = df.Loan_ID

df_test.head()

"""### *Saving the prediction file to 'output.csv'*"""

df_test = df_test[['Loan_ID', 'Loan_Status']].to_csv('output.csv',  index=True, header=True)

